{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8c56e1",
   "metadata": {},
   "source": [
    "# RAG Chatbot Pipeline â€“ Preprocessing, Tuning, and Evaluation\n",
    "Author: Rajesh Kumar Dogra\n",
    "\n",
    "This notebook demonstrates document processing, embedding generation, RAG pipeline execution, and evaluation using LangChain, FAISS, and HuggingFace models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Install necessary libraries\n",
    "!pip install -q langchain langchain-community sentence-transformers faiss-cpu transformers torch pypdf python-dotenv accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdd45f",
   "metadata": {},
   "source": [
    "## 1. Document Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b57333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "pdf_path = 'data/AI Training Document.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "text = \"\\n\".join([p.page_content.strip() for p in pages])\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_text(text)\n",
    "print(f\"âœ… Processed {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dabfec",
   "metadata": {},
   "source": [
    "## 2. Embedding & FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c59f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "vectorstore.save_local('vectordb/faiss_index')\n",
    "print(\"âœ… Vectorstore saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc0e64",
   "metadata": {},
   "source": [
    "## 3. Load LLM & Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad198e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "model_name = 'google/flan-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant answering questions based only on the context below.\\nIf the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer concisely:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec8b6e",
   "metadata": {},
   "source": [
    "## 4. Test Query & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is eBayâ€™s return policy?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(\"Answer:\", result['result'])\n",
    "print(\"\\nSources:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"Source {i}:\", doc.page_content[:300], \"...\\n\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}